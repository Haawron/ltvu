{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중복 제거 통계 내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_uid</th>\n",
       "      <th>num_caps</th>\n",
       "      <th>num_caps_comp</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bcfcd2b4-7ca3-45ea-bbeb-0c6bd1ddcaac</td>\n",
       "      <td>228</td>\n",
       "      <td>141</td>\n",
       "      <td>1433</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3cd6f6c1-b89f-4241-8b17-dc2dbd09d9e1</td>\n",
       "      <td>241</td>\n",
       "      <td>159</td>\n",
       "      <td>1716</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fcad17ad-ac2e-4d39-9a0f-da305e027ee7</td>\n",
       "      <td>241</td>\n",
       "      <td>177</td>\n",
       "      <td>1402</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7ba2bb39-2c15-42b5-990f-214ef1b730c3</td>\n",
       "      <td>241</td>\n",
       "      <td>177</td>\n",
       "      <td>2430</td>\n",
       "      <td>1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156f510b-d740-44e8-83a7-96af31eaad5a</td>\n",
       "      <td>241</td>\n",
       "      <td>177</td>\n",
       "      <td>1428</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>2ed232bc-dc21-42d6-88be-d6ebe92f5b2a</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>940</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>b5ae8df0-4825-4021-a01a-722947019865</td>\n",
       "      <td>241</td>\n",
       "      <td>203</td>\n",
       "      <td>2532</td>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>58fa07ae-2992-4dc9-842a-e5a73ee3d345</td>\n",
       "      <td>241</td>\n",
       "      <td>193</td>\n",
       "      <td>1624</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>d37ab6fe-4f57-41ef-b6ff-cb193be15303</td>\n",
       "      <td>241</td>\n",
       "      <td>138</td>\n",
       "      <td>1501</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>39f9a8a9-9979-475b-b4f9-1fa2eda064a4</td>\n",
       "      <td>240</td>\n",
       "      <td>178</td>\n",
       "      <td>1506</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1683 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  clip_uid  num_caps  num_caps_comp  \\\n",
       "0     bcfcd2b4-7ca3-45ea-bbeb-0c6bd1ddcaac       228            141   \n",
       "1     3cd6f6c1-b89f-4241-8b17-dc2dbd09d9e1       241            159   \n",
       "2     fcad17ad-ac2e-4d39-9a0f-da305e027ee7       241            177   \n",
       "3     7ba2bb39-2c15-42b5-990f-214ef1b730c3       241            177   \n",
       "4     156f510b-d740-44e8-83a7-96af31eaad5a       241            177   \n",
       "...                                    ...       ...            ...   \n",
       "1678  2ed232bc-dc21-42d6-88be-d6ebe92f5b2a       126             88   \n",
       "1679  b5ae8df0-4825-4021-a01a-722947019865       241            203   \n",
       "1680  58fa07ae-2992-4dc9-842a-e5a73ee3d345       241            193   \n",
       "1681  d37ab6fe-4f57-41ef-b6ff-cb193be15303       241            138   \n",
       "1682  39f9a8a9-9979-475b-b4f9-1fa2eda064a4       240            178   \n",
       "\n",
       "      num_tokens  num_tokens_comp  \n",
       "0           1433              950  \n",
       "1           1716             1191  \n",
       "2           1402             1076  \n",
       "3           2430             1797  \n",
       "4           1428             1070  \n",
       "...          ...              ...  \n",
       "1678         940              684  \n",
       "1679        2532             2232  \n",
       "1680        1624             1343  \n",
       "1681        1501              899  \n",
       "1682        1506             1142  \n",
       "\n",
       "[1683 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import T5Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "uuid_pattern = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}')\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "p_cap_dir = Path('../data/Ego4D-processed/captions/VideoRecap/caption_2s')\n",
    "records = []\n",
    "for i, p_cap in enumerate(p_cap_dir.glob('**/*.json')):\n",
    "    if not uuid_pattern.match(p_cap.stem):\n",
    "        continue\n",
    "    with open(p_cap, 'r') as f:\n",
    "        cap = json.load(f)\n",
    "    sr = pd.Series(cap['captions']['text'])\n",
    "    sr_comp = sr.loc[sr.shift(-1) != sr]\n",
    "    tokens = tokenizer(cap['captions']['text'], return_tensors='np')['attention_mask']\n",
    "    record = {\n",
    "        'clip_uid': cap['clip_uid'],\n",
    "        'num_caps': len(sr),\n",
    "        'num_caps_comp': len(sr_comp),\n",
    "        'num_tokens': sum(map(np.sum, tokens)),\n",
    "        'num_tokens_comp': sum(map(np.sum, tokens[sr_comp.index])),\n",
    "    }\n",
    "    records.append(record)\n",
    "df = pd.DataFrame(records)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_caps</th>\n",
       "      <th>num_caps_comp</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_tokens_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>1683.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>242.146168</td>\n",
       "      <td>177.080808</td>\n",
       "      <td>1897.291147</td>\n",
       "      <td>1468.387998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>78.839344</td>\n",
       "      <td>62.716351</td>\n",
       "      <td>739.637152</td>\n",
       "      <td>669.493362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>1408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>241.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>2145.000000</td>\n",
       "      <td>1783.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>601.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>7701.000000</td>\n",
       "      <td>6399.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          num_caps  num_caps_comp   num_tokens  num_tokens_comp\n",
       "count  1683.000000    1683.000000  1683.000000      1683.000000\n",
       "mean    242.146168     177.080808  1897.291147      1468.387998\n",
       "std      78.839344      62.716351   739.637152       669.493362\n",
       "min      10.000000       1.000000    79.000000         5.000000\n",
       "25%     241.000000     149.000000  1500.000000      1060.000000\n",
       "50%     241.000000     179.000000  1776.000000      1408.000000\n",
       "75%     241.000000     199.000000  2145.000000      1783.500000\n",
       "max     601.000000     547.000000  7701.000000      6399.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[['num_caps', 'num_caps_comp', 'num_tokens', 'num_tokens_comp']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER로 object, action 뽑기(를 위한 json 만들기)\n",
    "\n",
    "기본 구조\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"id\": \"CrossNER_AI_0\",\n",
    "        \"conversations\": [\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"Text: Typical generative model approaches include naive Bayes classifier s , Gaussian mixture model s , variational autoencoders and others .\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"I've read this text.\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"human\",\n",
    "                \"value\": \"What describes organization in the text?\"\n",
    "            },\n",
    "            {\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": \"[]\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62e15bbb06b4be6b48012ad274bf652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1685 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pattern_uuid = re.compile(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}')\n",
    "\n",
    "def generate_json_element(cap_id, cap_text, entity_type='object', **cap_info_kwargs):\n",
    "    return {\n",
    "        'id': cap_id,\n",
    "        'conversations': [\n",
    "            {\"from\": \"human\", \"value\": f\"Text: {cap_text} .\"},\n",
    "            {\"from\": \"gpt\",   \"value\": \"I've read this text.\"},\n",
    "            {\"from\": \"human\", \"value\": f\"What describes {entity_type} in the text?\"},\n",
    "            {\"from\": \"gpt\",   \"value\": \"[]\"}\n",
    "        ],\n",
    "        'entity_type': entity_type,\n",
    "        'info': {\n",
    "            **cap_info_kwargs,\n",
    "            'caption': cap_text,\n",
    "        }\n",
    "    }\n",
    "\n",
    "p_cap_dir = Path('../data/Ego4D-processed/captions/VideoRecap/caption_2s')\n",
    "p_out_inputs = p_cap_dir.with_name(p_cap_dir.stem + '_uniner_inputs')\n",
    "p_out_inputs.mkdir(exist_ok=True, parents=True)\n",
    "for p_cap in tqdm(list(p_cap_dir.glob('**/*.json'))):\n",
    "    if not pattern_uuid.match(p_cap.stem):\n",
    "        continue\n",
    "    with open(p_cap, 'r') as f:\n",
    "        cap = json.load(f)\n",
    "    df_cap = pd.DataFrame(cap['captions'])\n",
    "    df_cap = df_cap[df_cap['text'] != df_cap['text'].shift(1)]  # take first ones if consecutively same\n",
    "    duration = cap['video_end_sec'] - cap['video_start_sec']\n",
    "    df_cap['end'] = df_cap['start'].shift(-1, fill_value=duration)\n",
    "    df_cap = df_cap.reset_index(drop=True)\n",
    "    df_cap['text_orig'] = df_cap['text'].str.strip()\n",
    "    df_cap['text'] = df_cap['text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "    df_cap['text'] = df_cap['text'].str.replace(r'^[cC]\\s+', 'The camera wearer ', regex=True)\n",
    "    df_cap['text'] = df_cap['text'].str.replace(r'^#\\w\\s+', '', regex=True)\n",
    "    df_cap['cap_id'] = df_cap.index.map(str) + '_' + df_cap['start'].map('{:.0f}'.format) + '_' + df_cap['end'].map('{:.0f}'.format)\n",
    "    json_uniner_input = df_cap[['start', 'end', 'cap_id', 'text', 'text_orig']].apply(lambda row: generate_json_element(row['cap_id'], row['text'], caption_=re.sub(r'^[cC] ', '#C C ', row['text_orig']), start=row['start'], end=row['end']), axis=1).tolist()\n",
    "    p_out = p_out_inputs / (p_cap.stem + '.json')\n",
    "    with p_out.open('w') as f:\n",
    "        json.dump(json_uniner_input, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "srun -x 'ariel-v[10,12]' -p debug_grad -t 4:00:00 \\\n",
    "    --gres=gpu:1 \\\n",
    "    --cpus-per-gpu=8 \\\n",
    "    --mem-per-gpu=52G \\\n",
    "    --chdir=/data/gunsbrother/prjs/ltvu/llms/universal-ner \\\n",
    "    python -m src.eval.evaluate \\\n",
    "    --model_path Universal-NER/UniNER-7B-type \\\n",
    "    --data_path '/data/gunsbrother/prjs/ltvu/ours/notebooks/uniner_input.json' \\\n",
    "    --tensor_parallel_size 1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltvu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
