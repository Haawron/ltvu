{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/gunsbrother/prjs/ltvu/llms/Video-LLaVA\n"
     ]
    }
   ],
   "source": [
    "%cd /data/gunsbrother/prjs/ltvu/llms/Video-LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ariel-v8\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from videollava.conversation import conv_templates, SeparatorStyle\n",
    "from videollava.model.builder import load_pretrained_model\n",
    "from videollava.utils import disable_torch_init\n",
    "from videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c079383a2a2458c881d1fc616f948b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# setup\n",
    "disable_torch_init()\n",
    "model_path = 'LanguageBind/Video-LLaVA-7B'\n",
    "cache_dir = 'cache_dir'\n",
    "device = 'cuda'\n",
    "load_4bit, load_8bit = True, False\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)\n",
    "video_processor = processor['video']\n",
    "conv_mode = \"llava_v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg -i /data/datasets/ego4d_data/v2/clips_320p-non_official/0aca0078-b6ab-41fb-9dc5-a70b8ad137b2.mp4 -ss 0 -t 30 -c copy -avoid_negative_ts 1 -y /tmp/video-llava/0aca0078-b6ab-41fb-9dc5-a70b8ad137b2_000_030.mp4\n",
      "/tmp/video-llava/0aca0078-b6ab-41fb-9dc5-a70b8ad137b2_000_030.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def trim_video(clip_start_sec=0, clip_duration_sec=60):\n",
    "    video_path = Path('/data/datasets/ego4d_data/v2/clips_320p-non_official/0aca0078-b6ab-41fb-9dc5-a70b8ad137b2.mp4')\n",
    "    p_splitted_video_dir = Path('/tmp/video-llava')\n",
    "    p_splitted_video_dir.mkdir(exist_ok=True, parents=True)\n",
    "    clip_end_sec = clip_start_sec + clip_duration_sec\n",
    "    p_splitted_video = p_splitted_video_dir / f'{video_path.stem}_{clip_start_sec:03d}_{clip_end_sec:03d}{video_path.suffix}'\n",
    "    cmd = [\n",
    "        'ffmpeg',\n",
    "        '-i', str(video_path),\n",
    "        '-ss', str(clip_start_sec),\n",
    "        '-t', str(clip_duration_sec),\n",
    "        '-c', 'copy', '-avoid_negative_ts', '1', '-y',\n",
    "        str(p_splitted_video)\n",
    "    ]\n",
    "    print(' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    print(str(p_splitted_video))\n",
    "    return p_splitted_video\n",
    "\n",
    "# load the video\n",
    "START_SEC = 0\n",
    "DURATION_SEC = 30\n",
    "p_trimmed = trim_video(START_SEC, DURATION_SEC)\n",
    "video_tensor = video_processor(str(p_trimmed), return_tensors='pt')['pixel_values']\n",
    "if type(video_tensor) is list:\n",
    "    tensor = [video.to(model.device, dtype=torch.float16) for video in video_tensor]\n",
    "else:\n",
    "    tensor = video_tensor.to(model.device, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions:\n",
      "0.0s: The objects that help in answering the question \"where did I put the piece of timber?\" are a wooden floor and a wooden table.\n",
      "1.5s: The man is using a wooden floor and a wooden table to help him answer the question \"where did I put the piece of timber?\"\n",
      "3.0s: The man in the video is using a tool to push down on a board, which is a piece of timber. He is also using a hammer to hit the board, which is another piece of timber.\n",
      "4.5s: The man in the video is using a piece of paper to mark the location of the piece of timber.\n",
      "6.0s: The man is using a long metal instrument to sharpen a smaller metal instrument, which is the object that helps him answer the question \"where did I put the piece of timber?\"\n",
      "7.5s: The man in the video is using a stick to sharpen the knife, which can help him remember where he put the piece of timber.\n",
      "9.0s: The person is using a tool to push down on a piece of wood, which is the object that helps in answering the question \"where did I put the piece of timber?\"\n",
      "10.5s: The person in the video is using a wooden stick to mark the piece of wood, which helps in answering the question \"where did I put the piece of timber?\"\n",
      "12.0s: The person in the video is holding a wooden stick and a hammer, which can help in answering the question \"where did I put the piece of timber?\"\n",
      "13.5s: The person in the video uses a wooden stick and a knife to cut the wood, which can help in answering the question \"where did I put the piece of timber?\"\n",
      "15.0s: The person in the video is using a tool to sharpen the piece of wood, and there is a piece of wood on the floor next to them.\n",
      "16.5s: The person in the video uses a wooden stick to help them remember where they put the piece of timber.\n",
      "18.0s: The video shows a wooden box and a wooden table, which can help in answering the question \"where did I put the piece of timber?\"\n",
      "19.5s: The man is using a hand saw to cut the piece of wood, and he is also using a hammer to hit the wood. These objects can help in answering the question \"where did I put the piece of timber?\"\n",
      "21.0s: The man is using a wooden board to mark the piece of timber, and he is also using a wooden stick to measure the length of the timber.\n",
      "22.5s: The objects that help in answering the question \"where did I put the piece of timber?\" are a wooden table with a knife on it and a wooden chair with a knife on it.\n",
      "24.0s: The person is using a wooden stick to mark the piece of timber, which helps in answering the question \"where did I put the piece of timber?\"\n",
      "25.5s: The man is using a tool to shave the piece of wood, and he is also using a ruler to measure the wood. These objects help in answering the question \"where did I put the piece of timber?\"\n",
      "27.0s: The man in the video uses a wooden stick and a knife to help him answer the question \"where did I put the piece of timber?\"\n",
      "28.5s: The video shows a wooden box and a wooden table, which can help in answering the question \"where did I put the piece of timber?\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Captions:\n",
      "0.0s: The objects are a wooden floor and a wooden table.\n",
      "1.5s: the man is using a wooden floor and a wooden table\n",
      "3.0s: the man is using a tool to push down on a board, which is a piece of timber. He is also using a hammer to hit the board, which is another piece of timber.\n",
      "4.5s: the man is using a piece of paper to mark the location of the piece of timber.\n",
      "6.0s: the man is using a long metal instrument to sharpen a smaller metal instrument\n",
      "7.5s: the man is using a stick to sharpen the knife, which can help him remember where he put the piece of timber.\n",
      "9.0s: the man is using a tool to push down on a piece of wood\n",
      "10.5s: The person is using a wooden stick to mark the piece of wood\n",
      "12.0s: The person is holding a wooden stick and a hammer\n",
      "13.5s: The person uses a wooden stick and a knife to cut the wood\n",
      "15.0s: The person is using a tool to sharpen the piece of wood, and there is a piece of wood on the floor next to them.\n",
      "16.5s: The person uses a wooden stick to help them remember where they put the piece of timber.\n",
      "18.0s: The video shows a wooden box and a wooden table\n",
      "19.5s: the man is using a hand saw to cut the piece of wood, and he is also using a hammer to hit the wood. These objects can \n",
      "21.0s: the man is using a wooden board to mark the piece of timber, and he is also using a wooden stick to measure the length of the timber.\n",
      "22.5s: The objects are a wooden table with a knife on it and a wooden chair with a knife on it.\n",
      "24.0s: the man is using a wooden stick to mark the piece of timber\n",
      "25.5s: the man is using a tool to shave the piece of wood, and he is also using a ruler to measure the wood. These objects \n",
      "27.0s: the man uses a wooden stick and a knife\n",
      "28.5s: The video shows a wooden box and a wooden table\n"
     ]
    }
   ],
   "source": [
    "# load the caption\n",
    "SEP = '</s>'\n",
    "REPLACE_PATTERNS = [{\n",
    "    'replaced_with': 'the man',\n",
    "    'patterns': [\n",
    "        r'[Tt]he man',\n",
    "        r'[Tt]he person(?!(?:\\s+in))',  # not followed by in\n",
    "        r'[Tt]he person in this video',\n",
    "    ]}, {\n",
    "    'replaced_with': '',\n",
    "    'patterns': [\n",
    "        r'(, which.*| that.*| to )?help(s|ing)?.*answer.*\\\".*\\\"',\n",
    "        r' in the video',\n",
    "    ]}, {\n",
    "    'replaced_with': 'A',\n",
    "    'patterns': [\n",
    "        r'[Ii]n this video, we can see a',\n",
    "    ]}\n",
    "]\n",
    "\n",
    "with open(\"ltvu/captions/test/03_20240219v0/gathered.json\") as f:\n",
    "    data = json.load(f)\n",
    "q_inst = data['0aca0078-b6ab-41fb-9dc5-a70b8ad137b2']['q_instances']['9e5cd376-1b29-5861-8115-be750272d0a9']\n",
    "q_captions = q_inst['captions']\n",
    "caption_prompts = ['Captions:']\n",
    "for s, e, *_caps in q_captions:\n",
    "    if s < START_SEC: continue\n",
    "    if e > START_SEC + DURATION_SEC: break\n",
    "    _cap = _caps[-1]\n",
    "    _cap = f'{s}s: {_cap.replace(SEP, \"\")}'\n",
    "    caption_prompts.append(_cap)\n",
    "caption_prompt = '\\n'.join(caption_prompts)\n",
    "\n",
    "print(caption_prompt)\n",
    "for pattern_dict in REPLACE_PATTERNS:\n",
    "    repl = pattern_dict['replaced_with']\n",
    "    for pattern in pattern_dict['patterns']:\n",
    "        caption_prompt = re.sub(pattern, repl, caption_prompt)\n",
    "print('\\n\\n\\n')\n",
    "print(caption_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================== Round 0 ====================================\n",
      "------------------------------------- User -------------------------------------\n",
      "Captions:\n",
      "0.0s: The objects are a wooden floor and a wooden table.\n",
      "1.5s: the man is using a wooden floor and a wooden table\n",
      "3.0s: the man is using a tool to push down on a board, which is a piece of timber. He is also using a hammer to hit the board, which is another piece of timber.\n",
      "4.5s: the man is using a piece of paper to mark the location of the piece of timber.\n",
      "6.0s: the man is using a long metal instrument to sharpen a smaller metal instrument\n",
      "7.5s: the man is using a stick to sharpen the knife, which can help him remember where he put the piece of timber.\n",
      "9.0s: the man is using a tool to push down on a piece of wood\n",
      "10.5s: The person is using a wooden stick to mark the piece of wood\n",
      "12.0s: The person is holding a wooden stick and a hammer\n",
      "13.5s: The person uses a wooden stick and a knife to cut the wood\n",
      "15.0s: The person is using a tool to sharpen the piece of wood, and there is a piece of wood on the floor next to them.\n",
      "16.5s: The person uses a wooden stick to help them remember where they put the piece of timber.\n",
      "18.0s: The video shows a wooden box and a wooden table\n",
      "19.5s: the man is using a hand saw to cut the piece of wood, and he is also using a hammer to hit the wood. These objects can \n",
      "21.0s: the man is using a wooden board to mark the piece of timber, and he is also using a wooden stick to measure the length of the timber.\n",
      "22.5s: The objects are a wooden table with a knife on it and a wooden chair with a knife on it.\n",
      "24.0s: the man is using a wooden stick to mark the piece of timber\n",
      "25.5s: the man is using a tool to shave the piece of wood, and he is also using a ruler to measure the wood. These objects \n",
      "27.0s: the man uses a wooden stick and a knife\n",
      "28.5s: The video shows a wooden box and a wooden table\n",
      "\n",
      "Above is a sequence of short-term captions of this video generated by an video LLM after watching each short clip to later help another long-term LLM in answering the question \"where did I put the piece of timber?\". Each caption starts with its corresponding timestamp '${sec}s:'. What do you think is the most salient moment within this video for answering the question? And explain why.\n",
      "---------------------------------- Assistant -----------------------------------\n",
      "The most salient moment for answering the question \"where did I put the piece of timber?\" would be when the man uses a wooden stick to mark the location of the piece of timber. This is because the wooden stick is a tool specifically used for marking and measuring wood, and by using it to mark the location of the piece of timber, the man is able to remember where he put it. Additionally, the video shows the man using a hammer to hit the wood, which could potentially damage the piece of timber and make it difficult to remember where it is.</s>\n",
      "================================================================================\n",
      "\n",
      "=================================== Round 1 ====================================\n",
      "------------------------------------- User -------------------------------------\n",
      "What temporal window within this video does correspond to that moment? Tell me the start, and the end of that moment in seconds. Answer briefly.\n",
      "---------------------------------- Assistant -----------------------------------\n",
      "The moment when the man uses a wooden stick to mark the location of the piece of timber corresponds to the temporal window between 12.0s and 15.0s.</s>\n",
      "================================================================================\n",
      "\n",
      "=================================== Round 2 ====================================\n",
      "------------------------------------- User -------------------------------------\n",
      "The IoU score between your answer and the GT will be...?\n",
      "---------------------------------- Assistant -----------------------------------\n",
      "The IoU score between my answer and the ground truth will be 0.34.</s>\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"where did I put the piece of timber?\"\n",
    "prompts = [\n",
    "    '{captions}\\n\\n'\n",
    "    'Above is a sequence of short-term captions of this video generated by an video LLM after '\n",
    "    'watching each short clip to later help another long-term LLM in answering the question \"{query}\". '\n",
    "    'Each caption starts with its corresponding timestamp \\'${{sec}}s:\\'. '\n",
    "    'What do you think is the most salient moment within this video for answering the question? And explain why.',\n",
    "\n",
    "    'What temporal window within this video does correspond to that moment? '\n",
    "    'Tell me the start, and the end of that moment in seconds. '\n",
    "    # 'The temporal window can be longer than 1.5s. '\n",
    "    'Answer briefly.',\n",
    "\n",
    "    'The IoU score between your answer and the GT will be...?'\n",
    "    # 'Answer briefly.'\n",
    "]\n",
    "prompts = [\n",
    "    prompt.format(captions=caption_prompt, query=query)\n",
    "    for prompt in prompts\n",
    "]\n",
    "\n",
    "def converse(prompt, conv, model, tokenizer, tensor):\n",
    "    if conv.messages:\n",
    "        inp = prompt\n",
    "    else:\n",
    "        inp = ' '.join([DEFAULT_IMAGE_TOKEN] * model.get_video_tower().config.num_frames) + '\\n' + prompt\n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    keywords = [stop_str]\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=tensor,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            max_new_tokens=1024,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria])\n",
    "\n",
    "    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "    conv.messages[-1][-1] = outputs\n",
    "    return outputs\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f'{f\" Round {i} \":=^80s}')\n",
    "    print(f'{\" User \":-^80s}\\n{prompt}')\n",
    "    outputs = converse(prompt, conv, model, tokenizer, tensor)\n",
    "    print(f'{\" Assistant \":-^80s}\\n{outputs}')\n",
    "    print(f'{\"\":=^80s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts3 = [\n",
    "    'Here\\'are other debators\\' opinions:\\n'\n",
    "    '{debate}\\n\\n'\n",
    "    ''\n",
    "\n",
    "    f'Another discussant explained that the section from {debator1_start} seconds to {debator1_end} seconds of this video '\n",
    "    f'is appropriate for solving the question \"{query}\".\\n\\n'\n",
    "    f'\"{explanation}\"\\n\\n'\n",
    "    f'What do you think about this? The discussant has rated their answer with a score of {score} and assessed their '\n",
    "    f'confidence as {confidence}.',\n",
    "\n",
    "    f'Please discuss any shortcomings in the explanation given by the discussant while solving the problem {query}. '\n",
    "    f'The discussant\\'s explanation is as follows:\\n{explanation}',\n",
    "\n",
    "    f'You answered that the video section capable of solving \"{query}\" is from {debator2_start} seconds to '\n",
    "    f'{debator2_end} seconds. If you think a revision is necessary, please modify your answer.'\n",
    "]\n",
    "converse(prompt, conv, model, tokenizer, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "narr = json.load(open('/data/datasets/ego4d_data/v2/annotations/narration.json'))\n",
    "narr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narr[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
