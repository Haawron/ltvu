{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip UID: a769264b-e62e-4515-be98-a5828a533e45\n"
     ]
    }
   ],
   "source": [
    "p_nlq_val_json = Path('/data/gunsbrother/prjs/ltvu/llms/GroundVQA/data/unified/annotations.NLQ_val.json')\n",
    "nlq_val_data = json.load(p_nlq_val_json.open())\n",
    "clip_uids = set(entry['video_id'] for entry in nlq_val_data)\n",
    "clip_uid = np.random.choice(list(clip_uids))\n",
    "questions, gts = [], []\n",
    "for entry in nlq_val_data:\n",
    "\tif entry['video_id'] == clip_uid:\n",
    "\t\tquestions.append(entry['question'])\n",
    "\t\tgts.append((entry['clip_start_sec'], entry['clip_end_sec']))\n",
    "gts = np.array(gts)\n",
    "\n",
    "print(f'Clip UID: {clip_uid}')\n",
    "\n",
    "p_caps_dir = Path('/data/gunsbrother/prjs/ltvu/llms/LLaVA/results/egonlq/llava-v1.6-34b/global')\n",
    "p_cap = p_caps_dir / f'{clip_uid}.json'\n",
    "cap_data = json.load(p_cap.open())['answers']\n",
    "caps = [cap[-1] for cap in cap_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([44, 768]), torch.Size([12, 768]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_caps = model.encode(caps, convert_to_tensor=True, convert_to_numpy=False)\n",
    "z_qs = model.encode(questions, prompt='query: ', convert_to_tensor=True, convert_to_numpy=False)\n",
    "z_caps.shape, z_qs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3585, -0.4076, -0.3300,  ..., -0.0693, -0.0899, -0.1899],\n",
       "        [ 0.3008, -0.2176, -0.2229,  ...,  0.2494, -0.3454, -0.0568],\n",
       "        [ 0.3585, -0.4076, -0.3300,  ..., -0.0693, -0.0899, -0.1899],\n",
       "        ...,\n",
       "        [-0.0193, -0.4487, -0.2293,  ..., -0.0203, -0.0980,  0.0067],\n",
       "        [-0.0335, -0.4203, -0.1321,  ..., -0.1301, -0.3933, -0.0419],\n",
       "        [ 0.0033, -0.2894, -0.2347,  ..., -0.0927, -0.2202, -0.0320]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "sentence1_list = [\"My first sentence\", \"Another pair\"]\n",
    "sentence2_list = [\"My second sentence\", \"Unrelated sentence\"]\n",
    "labels_list = [0.8, 0.3]\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"sentence1\": sentence1_list,\n",
    "    \"sentence2\": sentence2_list,\n",
    "    \"label\": labels_list,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Another pair', 'sentence2': 'Unrelated sentence', 'label': 0.3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
