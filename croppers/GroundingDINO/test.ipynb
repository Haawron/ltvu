{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014210224151611328,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 440449768,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5922f34578364d36afa13de9f01254bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"weights/groundingdino_swinb_cogcoor.pth\")\n",
    "IMAGE_PATH = \".asset/cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading frames #     0 ~  1200\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/000\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/000/01199.png\n",
      "Reading frames #  1200 ~  2400\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/1200\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/1200/02399.png\n",
      "Reading frames #  2400 ~  3600\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/2400\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/2400/03599.png\n",
      "Reading frames #  3600 ~  4800\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/3600\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/3600/04799.png\n",
      "Reading frames #  4800 ~  6000\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/4800\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/4800/05999.png\n",
      "Reading frames #  6000 ~  7200\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/6000\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/6000/07199.png\n",
      "Reading frames #  7200 ~  8400\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/7200\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/7200/08399.png\n",
      "Reading frames #  8400 ~  9600\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/8400\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/8400/09599.png\n",
      "Reading frames #  9600 ~ 10800\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/9600\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/9600/10799.png\n",
      "Reading frames # 10800 ~ 12000\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/10800\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/10800/11999.png\n",
      "Reading frames # 12000 ~ 13200\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/12000\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/12000/13199.png\n",
      "Reading frames # 13200 ~ 14400\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/13200\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/13200/14399.png\n",
      "Reading frames # 14400 ~ 15600\n",
      "Chunk Dir: sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/14400\n",
      "saved sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/14400/14400.png\n"
     ]
    }
   ],
   "source": [
    "from decord import VideoReader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "p_clip = Path('sample/000eba33-8d14-446a-b016-19bd50e9a3b9/clip.mp4')\n",
    "vr = VideoReader(str(p_clip))\n",
    "\n",
    "chunk_size = 1200\n",
    "for chunk_idx, frame_offset in enumerate(range(0, len(vr), chunk_size)):\n",
    "    print(f'Reading frames # {frame_offset:5d} ~ {frame_offset + chunk_size:5d}')\n",
    "    x = vr[frame_offset : frame_offset + chunk_size].asnumpy()[..., ::-1]\n",
    "    p_frames_dir = p_clip.parent / f'rawframes/{chunk_idx:03d}'\n",
    "    p_frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f'Chunk Dir: {str(p_frames_dir)}')\n",
    "    for i, frame in enumerate(x):\n",
    "        global_frame_idx = frame_offset + i\n",
    "        p_frame = p_frames_dir / f'{global_frame_idx:05d}.png'\n",
    "        Image.fromarray(frame).save(p_frame)\n",
    "        print(f'\\rsaved {p_frame}', end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"weights/groundingdino_swinb_cogcoor.pth\")\n",
    "IMAGE_PATH = \"sample/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes/000/00000.png\"\n",
    "TEXT_PROMPT = \"chopping stick . dustbin . water bottle .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from groundingdino.models.GroundingDINO.groundingdino import GroundingDINO\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def fmts2hms(seconds) -> str:\n",
    "    seconds = float(seconds)\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f'{hours:2d} h {minutes:2d} m {seconds:2d} s'\n",
    "\n",
    "queries = [\n",
    "    'chopping stick',\n",
    "    'dustbin',\n",
    "    'water bottle',\n",
    "    'jar',\n",
    "    'chopsticks',\n",
    "    'bag',\n",
    "    'ceramic bowl',\n",
    "    'plates',\n",
    "    'piece of cloth',\n",
    "]\n",
    "\n",
    "device = f'cuda:0'\n",
    "model: GroundingDINO = load_model(\"groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"weights/groundingdino_swinb_cogcoor.pth\").to(device)\n",
    "# TEXT_PROMPT = \"chopping stick . dustbin . water bottle .\"\n",
    "TEXT_PROMPT = ' . '.join(queries) + ' .'\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "p_frames_dir = Path('samples/000eba33-8d14-446a-b016-19bd50e9a3b9/rawframes')\n",
    "p_all_frames = sorted(p_frames_dir.glob('**/*.png'))\n",
    "assert p_all_frames, f'No frames detected under {str(p_all_frames)}.'\n",
    "\n",
    "log_period = 100\n",
    "t0_global = time.time()\n",
    "t_loading, t_feeding = [], []\n",
    "print('Loading done. start processing. ...')\n",
    "for i, p_frame in enumerate(p_all_frames):\n",
    "    t0_frame = time.time()\n",
    "    image_source, image = load_image(str(p_frame))\n",
    "    t1_frame = time.time()\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=device\n",
    "    )\n",
    "    t2_frame = time.time()\n",
    "    t_loading.append(t1_frame - t0_frame)\n",
    "    t_feeding.append(t2_frame - t1_frame)\n",
    "    if i > 0 and i % log_period == 0:\n",
    "        print(f'[{i:5d}/{len(p_all_frames):5d}] Global {fmts2hms(t2_frame-t0_global)} | Loading {fmts2hms(sum(t_loading))} | Processing {fmts2hms(sum(t_feeding))}')\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "        cv2.imwrite(f'sample/000eba33-8d14-446a-b016-19bd50e9a3b9/annotated/{i:05d}.jpg', annotated_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBOX 다 뽑기\n",
    "\n",
    "- 매 비디오 → job\n",
    "    - 매 프레임 뭉텅이 → 한 번에 읽기도 무리 + 그렇다고 배치마다 읽기도 무리, 배치 사이즈 x N 배 수준으로\n",
    "        - 매 배치\n",
    "            ```python\n",
    "            frames = preprocess(vr[s:e].asarray())\n",
    "\n",
    "            annotations = ann[clip_id]\n",
    "            words = [ann['slot_x'] for ann in annotations]\n",
    "            query = ' . '.join(words) + ' .'\n",
    "            ```\n",
    "            - feed into the model\n",
    "\n",
    "- 이 레포 배치 여러 개에 대해 동작하도록 수정\n",
    "- 최대 배치사이즈?\n",
    "    ```python\n",
    "    B = 128; images = torch.randn(B, 3, 568, 320, pin_memory=True).to(device)\n",
    "    ```\n",
    "\n",
    "- annotation 뭉텅이:\n",
    "    ```python\n",
    "    clip_queries = {\n",
    "        \"clip_uid\": \"93231c7e-1cf4-4a20-b1f8-9cc9428915b2\",\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({322, 426, 568}, {320})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 비디오 사이즈는 3개밖에 없음\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'egonlq_clips_info.csv',\n",
    "    names=['clip_id', 'width', 'height', 'length'],\n",
    "    sep=' ', header=None)\n",
    "set(df['width']), set(df['height'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치사이즈 알아내기용 코드\n",
    "\n",
    "import torch\n",
    "from groundingdino.util.inference import load_model, predict\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinB_cfg.py\", \"weights/groundingdino_swinb_cogcoor.pth\")\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "device = 'cuda'\n",
    "B = 128; images = torch.randn(B, 3, 322, 320, pin_memory=True).to(device)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=images,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Union\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,\n",
       " 64,\n",
       " array([  19,   83,  147,  211,  275,  339,  403,  467,  531,  595,  659,\n",
       "         723,  787,  851,  915,  979, 1043, 1107, 1171, 1235, 1299, 1363,\n",
       "        1427, 1491, 1555, 1619]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global setup\n",
    "ngpus: int = 8\n",
    "workers_per_gpu: int = 8\n",
    "\n",
    "# ranks\n",
    "gpu_rank: int = 2  # [0, ngpus)\n",
    "worker_rank_offset: int = 3  # [0, workers_per_gpu), more like an offset\n",
    "num_workers: int = ngpus * workers_per_gpu\n",
    "worker_rank: int = gpu_rank * workers_per_gpu + worker_rank_offset  # [0, num_workers)\n",
    "\n",
    "# p_videos = Path('')\n",
    "# jobs: List[str] = sorted(p_videos)\n",
    "njobs = 1657#len(jobs)\n",
    "job_indices: List[str] = np.arange(worker_rank, njobs, num_workers)\n",
    "worker_rank, num_workers, job_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# clips: 1686, # queries: 18403\n",
      "Clip: 000eba33-8d14-446a-b016-19bd50e9a3b9, length: 14401\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/data/gunsbrother/repos/GroundingDINO/test.ipynb 셀 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m chunk: np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m vr[frame_offset : frame_offset \u001b[39m+\u001b[39m chunk_size]\u001b[39m.\u001b[39masnumpy()[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m frames \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39msplit(chunk, prefetch_factor):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     frames_source, frames \u001b[39m=\u001b[39m preprocess_video(frames, transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     boxes, logits, phrases \u001b[39m=\u001b[39m predict(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m         image\u001b[39m=\u001b[39mimages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m         text_threshold\u001b[39m=\u001b[39mTEXT_TRESHOLD\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     pprint(boxes)\n",
      "\u001b[1;32m/data/gunsbrother/repos/GroundingDINO/test.ipynb 셀 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m frames_transformed \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame \u001b[39min\u001b[39;00m frames:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     frame, frame_transformed \u001b[39m=\u001b[39m preprocess_frame(frame, transform)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     frames_transformed\u001b[39m.\u001b[39mappend(frame_transformed)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m frames_transformed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(frames_transformed)\n",
      "\u001b[1;32m/data/gunsbrother/repos/GroundingDINO/test.ipynb 셀 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_frame\u001b[39m(frame: np\u001b[39m.\u001b[39mndarray, transform) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     frame_transformed, _ \u001b[39m=\u001b[39m transform(frame, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bariel/data/gunsbrother/repos/GroundingDINO/test.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m frame, frame_transformed\n",
      "File \u001b[0;32m/data/gunsbrother/repos/GroundingDINO/groundingdino/datasets/transforms.py:302\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, image, target)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, image, target):\n\u001b[1;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m--> 302\u001b[0m         image, target \u001b[39m=\u001b[39m t(image, target)\n\u001b[1;32m    303\u001b[0m     \u001b[39mreturn\u001b[39;00m image, target\n",
      "File \u001b[0;32m/data/gunsbrother/repos/GroundingDINO/groundingdino/datasets/transforms.py:234\u001b[0m, in \u001b[0;36mRandomResize.__call__\u001b[0;34m(self, img, target)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img, target\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    233\u001b[0m     size \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msizes)\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mreturn\u001b[39;00m resize(img, target, size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size)\n",
      "File \u001b[0;32m/data/gunsbrother/repos/GroundingDINO/groundingdino/datasets/transforms.py:116\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, target, size, max_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         \u001b[39mreturn\u001b[39;00m get_size_with_aspect_ratio(image_size, size, max_size)\n\u001b[0;32m--> 116\u001b[0m size \u001b[39m=\u001b[39m get_size(image\u001b[39m.\u001b[39;49msize, size, max_size)\n\u001b[1;32m    117\u001b[0m rescaled_image \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mresize(image, size)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/gunsbrother/repos/GroundingDINO/groundingdino/datasets/transforms.py:114\u001b[0m, in \u001b[0;36mresize.<locals>.get_size\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m size[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m get_size_with_aspect_ratio(image_size, size, max_size)\n",
      "File \u001b[0;32m/data/gunsbrother/repos/GroundingDINO/groundingdino/datasets/transforms.py:91\u001b[0m, in \u001b[0;36mresize.<locals>.get_size_with_aspect_ratio\u001b[0;34m(image_size, size, max_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_size_with_aspect_ratio\u001b[39m(image_size, size, max_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 91\u001b[0m     w, h \u001b[39m=\u001b[39m image_size\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         min_original_size \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mmin\u001b[39m((w, h)))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from decord import VideoReader\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "\n",
    "def load_annotations():\n",
    "    p_ann_dir = Path('/data/datasets/ego4d_data/v2/annotations')\n",
    "    p_json_train, p_ann_val = p_ann_dir / 'nlq_train.json', p_ann_dir / 'nlq_val.json'\n",
    "    ann_train, ann_val = json.load(p_json_train.open()), json.load(p_ann_val.open())\n",
    "    ann_videos = ann_train['videos'] + ann_val['videos']\n",
    "    # 생김새: ann_videos[0]['clips'][0]['annotations'][0]['language_queries'][0]\n",
    "\n",
    "    # list of {'clip_uid': clip_uid, 'queries': list of query dicts}\n",
    "    ann_clips: List[Dict[str, Union[str,List[dict]]]] = []\n",
    "    clip_uid_to_idx: Dict[str, int] = {}\n",
    "    clip_idx = 0\n",
    "    for ann_video in ann_videos:\n",
    "        for ann_clip in ann_video['clips']:\n",
    "            clip_uid: str = ann_clip['clip_uid']\n",
    "            all_clip_language_queries: List[List[dict]] = [\n",
    "                annotation['language_queries']\n",
    "                for annotation in ann_clip['annotations']]\n",
    "            all_clip_language_queries: List[dict] = sum(\n",
    "                all_clip_language_queries, start=[])\n",
    "            ann_clips.append({'clip_uid': clip_uid, 'queries': all_clip_language_queries})\n",
    "            clip_uid_to_idx[clip_uid] = clip_idx\n",
    "            clip_idx += 1\n",
    "\n",
    "    n_clips = len(ann_clips)\n",
    "    n_queries = sum(map(lambda ann_clip: len(ann_clip['queries']), ann_clips))\n",
    "    print(f'# clips: {n_clips}, # queries: {n_queries}')  # 1686 (v1은 1326), 18403 (v1은 11291 + 3874 = 15165)\n",
    "    return ann_clips, clip_uid_to_idx\n",
    "\n",
    "def preprocess_frame(frame: np.ndarray, transform) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "    frame = np.asarray(frame)\n",
    "    frame_transformed, _ = transform(frame, None)\n",
    "    return frame, frame_transformed\n",
    "\n",
    "def preprocess_video(frames: np.ndarray, transform) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "    frames_transformed = []\n",
    "    for frame in frames:\n",
    "        frame, frame_transformed = preprocess_frame(frame, transform)\n",
    "        frames_transformed.append(frame_transformed)\n",
    "    frames_transformed = torch.cat(frames_transformed)\n",
    "    return frames, frames_transformed\n",
    "\n",
    "\n",
    "bsz = 128\n",
    "prefetch_factor = 16\n",
    "chunk_size = prefetch_factor * bsz\n",
    "ann_clips, clip_uid_to_idx = load_annotations()\n",
    "p_nlqv1_clips_dir = Path(f'/data/datasets/ego4d_data/v2/clips_320p-non_official')\n",
    "clip_uids: List[str] = [p_clip.stem for p_clip in p_nlqv1_clips_dir.glob('*.mp4')]\n",
    "transform = T.Compose(\n",
    "    [\n",
    "        T.RandomResize([800], max_size=1333),  # deterministic\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for clip_uid in clip_uids\n",
    "clip_uid = '000eba33-8d14-446a-b016-19bd50e9a3b9'\n",
    "p_clip = p_nlqv1_clips_dir / f'{clip_uid}.mp4'\n",
    "vr = VideoReader(str(p_clip))\n",
    "ann_clip = ann_clips[clip_uid_to_idx[clip_uid]]\n",
    "queries = ann_clip['queries']\n",
    "words = [query['slot_x'] for query in queries]\n",
    "text_prompt = ' . '.join(words) + ' .'  # 프롬프트는 클립 안에서는 동일함\n",
    "\n",
    "from pprint import pprint\n",
    "print(f'Clip: {clip_uid}, length: {len(vr)}')\n",
    "for chunk_idx, frame_offset in enumerate(range(0, len(vr), chunk_size)):\n",
    "    chunk: np.ndarray = vr[frame_offset : frame_offset + chunk_size].asnumpy()[..., ::-1]\n",
    "    for frames in np.split(chunk, prefetch_factor):\n",
    "        frames_source, frames = preprocess_video(frames, transform)\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=model,\n",
    "            image=images,\n",
    "            caption=text_prompt,\n",
    "            box_threshold=BOX_TRESHOLD,\n",
    "            text_threshold=TEXT_TRESHOLD\n",
    "        )\n",
    "        pprint(boxes)\n",
    "        pprint(logits)\n",
    "        pprint(phrases)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clip_start_sec': 17.25669,\n",
       "  'clip_end_sec': 27.256,\n",
       "  'video_start_sec': 17.2777186,\n",
       "  'video_end_sec': 27.2770286,\n",
       "  'video_start_frame': 518,\n",
       "  'video_end_frame': 818,\n",
       "  'template': 'Objects: What did I put in X?',\n",
       "  'query': 'what did I pick from the fridge?',\n",
       "  'slot_x': 'fridge',\n",
       "  'verb_x': 'pick',\n",
       "  'raw_tags': ['Objects: What did I put in X?',\n",
       "   'what did I pick from the fridge?',\n",
       "   'fridge',\n",
       "   'pick']},\n",
       " {'clip_start_sec': 56.73617,\n",
       "  'clip_end_sec': 59.932,\n",
       "  'video_start_sec': 56.7571986,\n",
       "  'video_end_sec': 59.9530286,\n",
       "  'video_start_frame': 1702,\n",
       "  'video_end_frame': 1798,\n",
       "  'template': 'Objects: What did I put in X?',\n",
       "  'query': 'what did I pick from the shelf?',\n",
       "  'slot_x': 'shelf',\n",
       "  'verb_x': 'pick',\n",
       "  'raw_tags': ['Objects: What did I put in X?',\n",
       "   'what did I pick from the shelf?',\n",
       "   'shelf',\n",
       "   'pick']},\n",
       " {'clip_start_sec': 123.17645,\n",
       "  'clip_end_sec': 124.176,\n",
       "  'video_start_sec': 123.1974786,\n",
       "  'video_end_sec': 124.1970286,\n",
       "  'video_start_frame': 3695,\n",
       "  'video_end_frame': 3725,\n",
       "  'template': 'Place: Where did I put X?',\n",
       "  'query': 'where did I put the egg shell?',\n",
       "  'slot_x': 'egg shell',\n",
       "  'verb_x': 'put',\n",
       "  'raw_tags': ['Place: Where did I put X?',\n",
       "   'where did I put the egg shell?',\n",
       "   'egg shell',\n",
       "   'put']},\n",
       " {'clip_start_sec': 139.21348,\n",
       "  'clip_end_sec': 141.213,\n",
       "  'video_start_sec': 139.2345086,\n",
       "  'video_end_sec': 141.2340286,\n",
       "  'video_start_frame': 4176,\n",
       "  'video_end_frame': 4236,\n",
       "  'template': 'Objects: What did I put in X?',\n",
       "  'query': 'what did I put in the fridge?',\n",
       "  'slot_x': 'fridge',\n",
       "  'verb_x': 'put',\n",
       "  'raw_tags': ['Objects: What did I put in X?',\n",
       "   'what did I put in the fridge?',\n",
       "   'fridge',\n",
       "   'put']},\n",
       " {'clip_start_sec': 145.53657,\n",
       "  'clip_end_sec': 149.536,\n",
       "  'video_start_sec': 145.5575986,\n",
       "  'video_end_sec': 149.5570286,\n",
       "  'video_start_frame': 4366,\n",
       "  'video_end_frame': 4486,\n",
       "  'template': 'Place: Where did I put X?',\n",
       "  'query': 'where did I pick the  chopstick?',\n",
       "  'slot_x': 'chopstick',\n",
       "  'verb_x': 'pick',\n",
       "  'raw_tags': ['Place: Where did I put X?',\n",
       "   'where did I pick the  chopstick?',\n",
       "   'chopstick',\n",
       "   'pick']},\n",
       " {'clip_start_sec': 334.2037,\n",
       "  'clip_end_sec': 335.203,\n",
       "  'video_start_sec': 334.22472860000005,\n",
       "  'video_end_sec': 335.2240286,\n",
       "  'video_start_frame': 10026,\n",
       "  'video_end_frame': 10056,\n",
       "  'template': 'Place: Where did I put X?',\n",
       "  'query': 'where did I put  the container?',\n",
       "  'slot_x': 'container',\n",
       "  'verb_x': 'put',\n",
       "  'raw_tags': ['Place: Where did I put X?',\n",
       "   'where did I put  the container?',\n",
       "   'container',\n",
       "   'put']},\n",
       " {'clip_start_sec': 419.41064,\n",
       "  'clip_end_sec': 425.41,\n",
       "  'video_start_sec': 419.4316686,\n",
       "  'video_end_sec': 425.43102860000005,\n",
       "  'video_start_frame': 12582,\n",
       "  'video_end_frame': 12762,\n",
       "  'template': 'Objects: What did I put in X?',\n",
       "  'query': 'what did I sprinkle on the cooking pan?',\n",
       "  'slot_x': 'cooking pan',\n",
       "  'verb_x': 'sprinkle',\n",
       "  'raw_tags': ['Objects: What did I put in X?',\n",
       "   'what did I sprinkle on the cooking pan?',\n",
       "   'cooking pan',\n",
       "   'sprinkle']},\n",
       " {'clip_start_sec': 0.0,\n",
       "  'clip_end_sec': 8.90145,\n",
       "  'video_start_sec': 0.0210286,\n",
       "  'video_end_sec': 8.9224786,\n",
       "  'video_start_frame': 0,\n",
       "  'video_end_frame': 267,\n",
       "  'template': 'Objects: Where is object X before / after event Y?',\n",
       "  'query': 'Where is the handle of frying pan before I turned on the cooker?',\n",
       "  'slot_x': 'Handle of frying pan',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'slot_y': 'I turned on the cooker',\n",
       "  'verb_y': 'turn-on',\n",
       "  'raw_tags': ['Objects: Where is object X before / after event Y?',\n",
       "   'Where is the handle of frying pan before I turned on the cooker?',\n",
       "   'Handle of frying pan',\n",
       "   '[verb_not_applicable]',\n",
       "   'I turned on the cooker',\n",
       "   'turn-on']},\n",
       " {'clip_start_sec': 4.06351,\n",
       "  'clip_end_sec': 15.41517,\n",
       "  'video_start_sec': 4.0845386,\n",
       "  'video_end_sec': 15.436198599999999,\n",
       "  'video_start_frame': 122,\n",
       "  'video_end_frame': 462,\n",
       "  'template': 'People: When did I talk to or interact with person with role X?',\n",
       "  'query': 'When did I rolled the sleeve of my left hand?',\n",
       "  'slot_x': 'left hand',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'raw_tags': ['People: When did I talk to or interact with person with role X?',\n",
       "   'When did I rolled the sleeve of my left hand?',\n",
       "   'left hand',\n",
       "   '[verb_not_applicable]']},\n",
       " {'clip_start_sec': 73.92416,\n",
       "  'clip_end_sec': 139.8603,\n",
       "  'video_start_sec': 73.9451886,\n",
       "  'video_end_sec': 139.8813286,\n",
       "  'video_start_frame': 2218,\n",
       "  'video_end_frame': 4196,\n",
       "  'template': 'Objects: How many X’s? (quantity question)',\n",
       "  'query': 'How many eggs did I fry?',\n",
       "  'slot_x': 'eggs',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'raw_tags': ['Objects: How many X’s? (quantity question)',\n",
       "   'How many eggs did I fry?',\n",
       "   'eggs',\n",
       "   '[verb_not_applicable]']},\n",
       " {'clip_start_sec': 138.24293,\n",
       "  'clip_end_sec': 142.33154,\n",
       "  'video_start_sec': 138.2639586,\n",
       "  'video_end_sec': 142.35256859999998,\n",
       "  'video_start_frame': 4147,\n",
       "  'video_end_frame': 4270,\n",
       "  'template': 'Objects: State of an object',\n",
       "  'query': 'When was the last time I closed the fridge?',\n",
       "  'slot_x': 'Fridge',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'raw_tags': ['Objects: State of an object',\n",
       "   'When was the last time I closed the fridge?',\n",
       "   'Fridge',\n",
       "   '[verb_not_applicable]']},\n",
       " {'clip_start_sec': 288.02578,\n",
       "  'clip_end_sec': 473.48027,\n",
       "  'video_start_sec': 288.0468086,\n",
       "  'video_end_sec': 473.50129860000004,\n",
       "  'video_start_frame': 8641,\n",
       "  'video_end_frame': 14204,\n",
       "  'template': 'Objects: State of an object',\n",
       "  'query': 'Did I turn off the cooker after I fried the meat?',\n",
       "  'slot_x': 'cooker',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'slot_y': 'I fried the meat',\n",
       "  'verb_y': 'fry',\n",
       "  'raw_tags': ['Objects: State of an object',\n",
       "   'Did I turn off the cooker after I fried the meat?',\n",
       "   'cooker',\n",
       "   '[verb_not_applicable]',\n",
       "   'I fried the meat',\n",
       "   'fry']},\n",
       " {'clip_start_sec': 292.36192,\n",
       "  'clip_end_sec': 307.72595,\n",
       "  'video_start_sec': 292.3829486,\n",
       "  'video_end_sec': 307.74697860000003,\n",
       "  'video_start_frame': 8771,\n",
       "  'video_end_frame': 9232,\n",
       "  'template': 'Objects: How many X’s? (quantity question)',\n",
       "  'query': 'How many meat slice did I fry?',\n",
       "  'slot_x': 'Meat slice',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'raw_tags': ['Objects: How many X’s? (quantity question)',\n",
       "   'How many meat slice did I fry?',\n",
       "   'Meat slice',\n",
       "   '[verb_not_applicable]']},\n",
       " {'clip_start_sec': 338.97823,\n",
       "  'clip_end_sec': 373.46524,\n",
       "  'video_start_sec': 338.9992586,\n",
       "  'video_end_sec': 373.4862686,\n",
       "  'video_start_frame': 10169,\n",
       "  'video_end_frame': 11204,\n",
       "  'template': 'Objects: Where is object X before / after event Y?',\n",
       "  'query': 'Where did I put the ceramic mug after I washed it?',\n",
       "  'slot_x': 'Ceramic Mug',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'slot_y': 'I washed it',\n",
       "  'verb_y': 'wash',\n",
       "  'raw_tags': ['Objects: Where is object X before / after event Y?',\n",
       "   'Where did I put the ceramic mug after I washed it?',\n",
       "   'Ceramic Mug',\n",
       "   '[verb_not_applicable]',\n",
       "   'I washed it',\n",
       "   'wash']},\n",
       " {'clip_start_sec': 408.26236,\n",
       "  'clip_end_sec': 425.88745,\n",
       "  'video_start_sec': 408.2833886,\n",
       "  'video_end_sec': 425.9084786,\n",
       "  'video_start_frame': 12248,\n",
       "  'video_end_frame': 12777,\n",
       "  'template': 'Objects: State of an object',\n",
       "  'query': 'Did I add sauce to the meat?',\n",
       "  'slot_x': 'Sauce',\n",
       "  'verb_x': '[verb_not_applicable]',\n",
       "  'raw_tags': ['Objects: State of an object',\n",
       "   'Did I add sauce to the meat?',\n",
       "   'Sauce',\n",
       "   '[verb_not_applicable]']}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_queries[list(clip_queries.keys())[0]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
